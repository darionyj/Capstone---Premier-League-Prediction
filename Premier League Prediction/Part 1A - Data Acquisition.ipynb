{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: English Premier League Predictions\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Football is the most [popular sport](http://www.biggestglobalsports.com/) in the world. It is mostly dominant in Europe, South America and Africa. Most of the top footballers are from Europe and South America. As most of the past world cup winners were from these two continents. Brazil has won the most time which is 5 times. Other countries that won before includes Germany, Italy, Argentina, France, Spain, England and Uruguay. \n",
    "\n",
    "In the different countries, each have their individual football leagues. One country can have more than one league and each league usually have 20 teams. Each league in a country is connected to one another. An example will be the English Premier League. It has 20 teams every season and whenever the season ends, the bottom three teams will be relegated to the league below them, which is the English Football League. Then, the top three teams will be promoted to the English Premier League. It is the same for the English Football League where the teams were relegated, and there are many leagues below them too.\n",
    "\n",
    "[English Premier League](https://www.premierleague.com/home) is one of the top few leagues in the world. It was originally founded as Football League Division One in 1888, and broke away from the Football League in 1992, forming the Premier League. Notable teams in the league are Manchester United, Arsenal, Chelsea, Manchester City and Liverpool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement \n",
    "\n",
    "We will be predicting the football matches in the English Premier League. As football matches are always full of surprise, we will be using past matches data and FIFA game series data. With these data and analysis, the classification model can assist potential stakeholders such as football pundits, betting website, football fans and shareholders of football clubs. \n",
    "\n",
    "As mentioned, we will be using data from past matches and the FIFA game series.\n",
    "\n",
    "FIFA is a football game which has all the team stats and player stats. With these data and analysis, we can better understand the relationship between the result and the match stats. As football is associated with betting, we will also compare the predictions with odds from [Singapore Pools](https://online.singaporepools.com/en/sports/competition/36/football/england/english-premier).\n",
    "\n",
    "There will be a total of seven notebooks:\n",
    "\n",
    "1) Part 1A - Data Acquisition (This notebook)\n",
    "\n",
    "2) Part 1B - FIFA & Season Fixtures Clean Up\n",
    "\n",
    "3) Part 2 - Data Preparation, EDA & Feature Engineering\n",
    "\n",
    "4) Part 3 - Modeling for Result\n",
    "\n",
    "5) Part 4A - Modeling for Home Total Goals\n",
    "\n",
    "6) Part 4B - Modeling for Away Total Goals\n",
    "\n",
    "7) Part 5 - Predictions & Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1A - Data Acquisition \n",
    "\n",
    "There will be two type of data used. One is match fixtures for past seasons and current seasons, the other will be FIFA players stats for each season. We will be looking at seasons starting from 2017/2018 to current, 2020/2021.\n",
    "\n",
    "The match fixtures data will be scraped from [FBREF](https://fbref.com/en/). For the FIFA players stats, the data will be taken from [Kaggle](https://www.kaggle.com/stefanoleone992/fifa-21-complete-player-dataset?select=players_21.csv). \n",
    "\n",
    "\n",
    "Match reports will be scraped from each season into a folder each. Then, a function will be used to combine all csv inside the folder into one. \n",
    "\n",
    "A separate dataset will be scraped for model predictions. It will be scrapped from the current season.\n",
    "\n",
    "### Contents:\n",
    " - [Functions for Web Scraping](#-Functions-for-Web-Scraping)\n",
    " - [Web Scraping from FBREF](#-Web-Scraping-from-FBREF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of Modules\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display of rows and columns\n",
    "pd.set_option('max_rows',None)\n",
    "pd.set_option('max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoresfixtures(link,ids):\n",
    "    '''\n",
    "    Description: This function picks all the games that had in one season and combinate all links to one especific list\n",
    "    \n",
    "    Inputs:\n",
    "        - link: The link of the main page that have all season games desired.\n",
    "        - ids: The ID of the championship table\n",
    "        \n",
    "    Outputs:\n",
    "        - specific list that has all the links os all matches of the season\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    req = requests.get(link)\n",
    "    if req.status_code == 200:\n",
    "        content = req.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    tb = soup.find(id=ids)\n",
    "\n",
    "    s1= []\n",
    "    s2= []\n",
    "    for i in tb.find_all(\"a\"):\n",
    "            s1.append(str(i))\n",
    "            s2.append(str(i.get_text('href')))\n",
    "\n",
    "\n",
    "    # Calling DataFrame constructor after zipping \n",
    "    # both lists, with columns specified \n",
    "    di = pd.DataFrame(list(zip(s1, s2)), \n",
    "                   columns =['Codes', 'ID']) \n",
    "\n",
    "    s4=[]\n",
    "    for i in di[\"Codes\"]:\n",
    "        i = i.replace('<a href=\"','')\n",
    "        i = i.replace('</a>','')\n",
    "        s4.append(str(i))\n",
    "\n",
    "\n",
    "    s5 = []\n",
    "\n",
    "    for i in di['Codes']:\n",
    "        if \"matches\" in i:\n",
    "            s5.append(str(i))\n",
    "        else:\n",
    "            s5.append(0)\n",
    "\n",
    "    s6 = []\n",
    "    for i in di[\"Codes\"]:\n",
    "        if '<a href=\"/en/squads/' in i:\n",
    "            i = i.replace('<a href=\"/en/squads/','')\n",
    "            i = i[0:8]\n",
    "            s6.append(str(i))\n",
    "        else:\n",
    "            s6.append(0)        \n",
    "\n",
    "    # Calling DataFrame constructor after zipping \n",
    "    # both lists, with columns specified \n",
    "    da = pd.DataFrame(list(zip(s1, s2,s4,s5,s6)), \n",
    "                   columns =['CODES', 'ID','URL_FINAL','GAMES_2020',\"TEAM_CODE\"])        \n",
    "\n",
    "    s9 = []\n",
    "    for i in da[\"URL_FINAL\"]:\n",
    "        if 'Match Report' in i:\n",
    "            s9.append(str(i))\n",
    "        else:\n",
    "            pass\n",
    "    return s9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_report(url,folder_name):\n",
    "    '''\n",
    "    Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
    "    \n",
    "    Input:\n",
    "        - url: Url of the html page\n",
    "        \n",
    "    Output:\n",
    "        - Dataframe will be saved as csv\n",
    "    \n",
    "    '''\n",
    "    #make the request\n",
    "    pg = 'https://fbref.com'\n",
    "    url_pg = pg+ url\n",
    "    req = requests.get(url_pg)\n",
    "    if req.status_code == 200:\n",
    "        content = req.content\n",
    "    #accessing data from site\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    table_general = soup.find_all(class_ = \"table_container\")\n",
    "    #Some match reports have lesser data, therefore, the table is lesser\n",
    "    if len(table_general) <= 2:\n",
    "        table_team_1 = table_general[0]\n",
    "        table_team_2 = table_general[1]\n",
    "        table_team_5 = soup.find(class_='venuetime')\n",
    "    elif len(table_general) > 4:\n",
    "        table_team_1 = table_general[0]\n",
    "        table_team_2 = table_general[7]\n",
    "        table_team_3 = table_general[5]\n",
    "        table_team_4 = table_general[12]\n",
    "        table_team_5 = soup.find(class_='venuetime')\n",
    "    else:\n",
    "        table_team_1 = table_general[0]\n",
    "        table_team_2 = table_general[2]\n",
    "        table_team_5 = soup.find(class_='venuetime')\n",
    "\n",
    "    #collecting data\n",
    "    table_match = soup.find_all('div',class_ = \"scorebox_meta\")\n",
    "    oi_match = str(table_match)\n",
    "\n",
    "    #treating data\n",
    "    toby = oi_match.split('<small>')\n",
    "    match = str(toby[2])\n",
    "    match = match.split('</small>')\n",
    "    match = str(match[0])\n",
    "    stadium = toby[4]\n",
    "    stadium = stadium.split('</small>')\n",
    "    stadium = str(stadium[0])    \n",
    "    \n",
    "    #collecting data\n",
    "    date = table_team_5.get('data-venue-date')        \n",
    "\n",
    "    #treating data\n",
    "    name = str(soup.title)\n",
    "    name = name.replace(\" \",\"_\")\n",
    "    name = name.replace(\"<title>\",\"\")\n",
    "    name = name.replace(\".\",\"\")\n",
    "    name_final = name.split(\"Report\")[0]\n",
    "\n",
    "\n",
    "    #treating data\n",
    "    name_final = name_final.split(\"_Match\")\n",
    "    name_final = name_final[0]    \n",
    "\n",
    "    # STR transform and reading tables\n",
    "    if len(table_general) <= 2:\n",
    "        table_str_1 = str(table_team_1)\n",
    "        table_str_2 = str(table_team_2)\n",
    "    elif len(table_general) > 4:\n",
    "        table_str_1 = str(table_team_1)\n",
    "        table_str_2 = str(table_team_2)\n",
    "        table_str_3 = str(table_team_3)\n",
    "        table_str_4 = str(table_team_4)\n",
    "        df_1 = pd.read_html(table_str_1)[0]\n",
    "        df_2 = pd.read_html(table_str_2)[0]\n",
    "        df_3 = pd.read_html(table_str_3)[0]\n",
    "        df_4 = pd.read_html(table_str_4)[0]\n",
    "    else:\n",
    "        table_str_1 = str(table_team_1)\n",
    "        table_str_2 = str(table_team_2)\n",
    "        df_1 = pd.read_html(table_str_1)[0]\n",
    "        df_2 = pd.read_html(table_str_2)[0]\n",
    "\n",
    "    #treating data\n",
    "    time = str(name_final)\n",
    "    time = time.replace(\"_\",\" \")\n",
    "    time = time.split(\" vs \")\n",
    "    time_1 = str(time[0])\n",
    "    time_2 = str(time[1])\n",
    "\n",
    "    #Dtframe transforming\n",
    "    df_1 = pd.DataFrame(df_1)\n",
    "    df_1.columns = df_1.columns.droplevel()\n",
    "    df_1['Home'] = str(time_1)\n",
    "    df_1['Away'] = str(time_2)\n",
    "    df_1['Match'] = str(name_final)\n",
    "    df_1['Date'] = str(date)\n",
    "    df_1['Stadium'] = str(stadium)\n",
    "    df_1['Attendance'] = match\n",
    "    if len(table_general) > 4:\n",
    "        df_3.columns = df_3.columns.droplevel()\n",
    "        df_3.drop(columns=['Player', '#', 'Nation', 'Pos', 'Age', 'Min', 'CrdY', 'CrdR'],inplace=True)\n",
    "        df_5 = pd.merge(df_1,df_3,left_index=True,right_index=True)\n",
    "    else:\n",
    "        df_5 = df_1\n",
    "\n",
    "    df_2 = pd.DataFrame(df_2)\n",
    "    df_2.columns = df_2.columns.droplevel()\n",
    "    df_2['Home'] = str(time_2)\n",
    "    df_2['Away'] = str(time_1)\n",
    "    df_2['Match'] = str(name_final)\n",
    "    df_2['Date'] = str(date)\n",
    "    df_2['Stadium'] = str(stadium)\n",
    "    df_2['Attendance'] = match\n",
    "    if len(table_general) > 4:\n",
    "        df_4.columns = df_4.columns.droplevel()\n",
    "        df_4.drop(columns=['Player', '#', 'Nation', 'Pos', 'Age', 'Min', 'CrdY', 'CrdR'],inplace=True)\n",
    "        df_6 = pd.merge(df_2,df_4,left_index=True,right_index=True)\n",
    "    else:\n",
    "        df_6 = df_2    \n",
    "    \n",
    "    #APPENDING Dataframes\n",
    "    df_8 = df_5.append(df_6)\n",
    "    df_8.to_csv(f'data/{folder_name}/{name_final}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(direct,name):\n",
    "    '''\n",
    "    Description: This function is to combine all csv file into one in a folder\n",
    "    \n",
    "    Input:\n",
    "        - direct - folder location\n",
    "        - name - new csv file name\n",
    "        \n",
    "    Output:\n",
    "        - All csv will be combined into a new csv\n",
    "    \n",
    "    '''\n",
    "    os.chdir(direct)\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    combined_csv.to_csv(name,index=False,encoding ='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping from FBREF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 2017/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "\n",
    "#for url in scoresfixtures(\"https://fbref.com/en/comps/9/1631/schedule/2017-2018-Premier-League-Scores-and-Fixtures\",\n",
    "#                          \"div_sched_ks_1631_1\"):\n",
    "#       match_report(url,'epl2018')\n",
    "\n",
    "#print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scrap for EPL\n",
    "#combine_csv(\"data/epl2018\",'epl2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 2018/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "\n",
    "#for url in scoresfixtures(\"https://fbref.com/en/comps/9/1889/schedule/2018-2019-Premier-League-Scores-and-Fixtures\",\n",
    "#                          \"div_sched_ks_1889_1\"):\n",
    "#       match_report(url,'epl2019')\n",
    "\n",
    "#print('Duration: {} seconds'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scrap for EPL\n",
    "#combine_csv(\"data/epl2019\",'epl2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 2019/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "\n",
    "#for url in scoresfixtures(\"https://fbref.com/en/comps/9/3232/schedule/2019-2020-Premier-League-Scores-and-Fixtures\",\n",
    "#                          \"div_sched_ks_3232_1\"):\n",
    "#       match_report(url,'epl2020')\n",
    "\n",
    "#print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scrap for EPL\n",
    "#combine_csv(\"data/epl2020\",'epl2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 2020/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 98.67593288421631 seconds\n"
     ]
    }
   ],
   "source": [
    "#Web Scrap for epl 2021 fixtures\n",
    "#start = time.time()\n",
    "\n",
    "#for url in scoresfixtures(\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\",\n",
    "#                          \"div_sched_ks_10728_1\"):\n",
    "#       match_report(url,'epl2021')\n",
    "\n",
    "#print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_csv(\"data/epl2021\",'epl2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Set\n",
    "\n",
    "This is also the Season 2020/2021 dataset, however it will be updated now and then for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 524.2856922149658 seconds\n"
     ]
    }
   ],
   "source": [
    "#Web Scrap for epl 2021 fixtures\n",
    "#start = time.time()\n",
    "\n",
    "#for url in scoresfixtures(\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\",\n",
    "#                          \"div_sched_ks_10728_1\"):\n",
    "#       match_report(url,'epl2021_prediction')\n",
    "\n",
    "#print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_csv(\"data/epl2021_prediction\",'epl2021_predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
